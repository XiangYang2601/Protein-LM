{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "import time\n",
    "import scipy\n",
    "import time\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Supplementary functions'''\n",
    "\n",
    "def extract_pdb_info(pdb_file_path):\n",
    "    \"\"\"\n",
    "    Reads a PDB file and extracts 3D spatial coordinates and amino acids.\n",
    "\n",
    "    Input: pdb_file_path (str): Path to the PDB file.\n",
    "\n",
    "    Output:  A list of dictionaries, each containing information about each atom.\n",
    "    \"\"\"\n",
    "    # Create a PDBParser object\n",
    "    parser = PDBParser(PERMISSIVE=1)\n",
    "    \n",
    "    # Parse the structure from the PDB file\n",
    "    structure = parser.get_structure('protein', pdb_file_path)\n",
    "    \n",
    "    # List to hold the extracted information\n",
    "    atom_info_list = []\n",
    "\n",
    "    # Extract information from the structure\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    atom_info = {\n",
    "                        'model_id': model.id,\n",
    "                        'chain_id': chain.id,\n",
    "                        'residue_name': residue.resname,\n",
    "                        'residue_id': residue.id[1],\n",
    "                        'atom_name': atom.name,\n",
    "                        'atom_coords': atom.coord.tolist()\n",
    "                    }\n",
    "                    atom_info_list.append(atom_info)\n",
    "\n",
    "    return atom_info_list\n",
    "\n",
    "def find_nearest_neighbors(data, angstrom):\n",
    "    \"\"\"\n",
    "    Find the n-neighbors closest to the 'CA' atoms in the data using brute force with Euclidean distance.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of dictionaries, each containing atom information\n",
    "    - angstrom: float, distance threshold for neighbor counting\n",
    "\n",
    "    Returns:\n",
    "    - data: list of dictionaries, containing only 'CA' atoms with updated 'neighbour_count' field,\n",
    "      and without 'model_id', 'atom_name', and 'atom_coords' keys.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Filter data to only include 'CA' atoms in chain_id 'A'\n",
    "    ca_atoms = [atom for atom in data if atom.get('chain_id') == 'A' and atom['atom_name'] == 'CA']\n",
    "    ca_coords = np.array([atom['atom_coords'] for atom in ca_atoms])\n",
    "\n",
    "    # Calculate distances between 'CA' atoms\n",
    "    distances = scipy.spatial.distance.cdist(ca_coords, ca_coords, 'euclidean')\n",
    "\n",
    "    # Use 'residue_id' for DataFrame indexing\n",
    "    residue_ids = [atom.get('residue_id', f\"residue_{i}\") for i, atom in enumerate(ca_atoms)]\n",
    "    \n",
    "    # Convert distance matrix to pandas DataFrame\n",
    "    distance_df = pd.DataFrame(distances, index=residue_ids, columns=residue_ids)\n",
    "\n",
    "    # Format distance matrix to 3 decimal places\n",
    "    distance_df = distance_df.round(3)\n",
    "\n",
    "    # Print the distance matrix\n",
    "    print(\"Distance matrix between CA atoms (formatted to 3 decimal places):\")\n",
    "    print(distance_df)\n",
    "\n",
    "    # Count neighbors within a distance of 'angstrom' units for each 'CA' atom\n",
    "    for index, ca_point in enumerate(ca_coords):\n",
    "        neighbour_count = np.sum((distances[index] <= angstrom) & (distances[index] != 0))\n",
    "        ca_atoms[index]['neighbour_count'] = neighbour_count\n",
    "\n",
    "    # Remove specified keys from the 'CA' atoms\n",
    "    keys_to_remove = ['model_id', 'atom_name', 'atom_coords']\n",
    "    for atom in ca_atoms:\n",
    "        for key in keys_to_remove:\n",
    "            atom.pop(key, None)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed:.2f} seconds\")\n",
    "\n",
    "    return ca_atoms\n",
    "\n",
    "def extract_conservation_score(filename):\n",
    "    result = []\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                # Skip header lines and lines below confidence cut-off\n",
    "                if re.match(r'\\s*\\d+', line):\n",
    "                    columns = line.split()\n",
    "                    pos = int(columns[0])\n",
    "                    score = float(columns[3])\n",
    "\n",
    "                    # Check if there is a 3LATOM value, otherwise continue\n",
    "                    if len(columns) > 6 and ':' in columns[2]:\n",
    "                        residue_id = re.findall(r'\\d+', columns[2])[0]  # Extract numbers from the string\n",
    "                        residue_id = int(residue_id)  # Convert extracted string to integer\n",
    "                        residue_name = re.findall(r'[A-Za-z]+', columns[2])[0]  # Extract letters from the string\n",
    "                        result.append({'residue_id': residue_id, 'residue_name': residue_name, 'conservation_score': score})\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def merge_residue_data(conservation_scores, neighbour_counts):\n",
    "    # Create a dictionary to index the neighbour_counts by residue_id and residue_name\n",
    "    neighbour_dict = {(item['residue_id'], item['residue_name']): item for item in neighbour_counts}\n",
    "    \n",
    "    # List to hold merged data\n",
    "    merged_data = []\n",
    "    \n",
    "    # Merge data from conservation_scores with neighbour_counts\n",
    "    for score_entry in conservation_scores:\n",
    "        residue_id = score_entry['residue_id']\n",
    "        residue_name = score_entry['residue_name']\n",
    "        \n",
    "        # Find the corresponding entry in neighbour_counts\n",
    "        neighbour_entry = neighbour_dict.get((residue_id, residue_name))\n",
    "        \n",
    "        if neighbour_entry:\n",
    "            # Merge the two entries\n",
    "            merged_entry = {\n",
    "                'residue_id': residue_id,\n",
    "                'residue_name': residue_name,\n",
    "                'conservation_score': score_entry['conservation_score'],\n",
    "                'neighbour_count': neighbour_entry['neighbour_count']\n",
    "            }\n",
    "            merged_data.append(merged_entry)\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def plot_and_calculate_correlation(data, title, num_residues=1600):\n",
    "    # Extract data for plotting\n",
    "    residue_ids = [entry['residue_id'] for entry in data]\n",
    "    neighbour_counts = [entry['neighbour_count'] for entry in data]\n",
    "    \n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot neighbour counts\n",
    "    plt.scatter(residue_ids, neighbour_counts, color='green', label='Neighbour Count')\n",
    "    \n",
    "    # Check and plot conservation scores if available\n",
    "    conservation_scores = [entry['conservation_score'] for entry in data if 'conservation_score' in entry and entry['conservation_score'] is not None]\n",
    "    if conservation_scores:\n",
    "        valid_entries = [entry for entry in data if 'conservation_score' in entry and entry['conservation_score'] is not None]\n",
    "        valid_residue_ids = [entry['residue_id'] for entry in valid_entries]\n",
    "        valid_neighbour_counts = [entry['neighbour_count'] for entry in valid_entries]\n",
    "        \n",
    "        plt.scatter(valid_residue_ids, conservation_scores, color='blue', label='Conservation Score')\n",
    "        \n",
    "        # Calculate and print the correlation\n",
    "        if len(valid_residue_ids) == len(conservation_scores):\n",
    "            correlation = np.corrcoef(conservation_scores, valid_neighbour_counts)[0, 1]\n",
    "            print(f\"Correlation between Conservation Score and Neighbour Count: {correlation}\")\n",
    "        else:\n",
    "            print(\"Mismatch in lengths of valid_residue_ids and conservation_scores\")\n",
    "    else:\n",
    "        print(\"Conservation scores not found. Plotting only Neighbour Count.\")\n",
    "    \n",
    "    plt.xlabel('Residue ID')\n",
    "    plt.ylabel('Score / Count')\n",
    "    plt.title(f'{title}: Residue ID vs Conservation Score and Neighbour Count ({angstrom}A)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set plot limits for residue IDs\n",
    "    plt.xlim(0, num_residues)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "def download_conservation_scores(pdb_file_path, chains):\n",
    "    pattern = r'pdb(\\w+)\\.pdb'\n",
    "    pdb_ids = [match.group(1).upper() for file_path in pdb_file_path if (match := re.search(pattern, file_path))]\n",
    "    print(pdb_ids)\n",
    "    \n",
    "    # Ensure the directory for saving files exists\n",
    "    output_dir = \"conservation_scores\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    async def scrape_consurf(pdb_id, chain):\n",
    "        url = f\"https://consurfdb.tau.ac.il/DB/{pdb_id.upper()}{chain}/{pdb_id.upper()}{chain}_consurf_summary.txt\"\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch()\n",
    "            page = await browser.new_page()\n",
    "            try:\n",
    "                await page.goto(url, timeout=60000)  # Increase timeout to 60 seconds\n",
    "                content = await page.content()  # Get the content of the page\n",
    "                if \"Page not found\" in content:\n",
    "                    print(f\"Error: Page not found for {pdb_id} chain {chain}\")\n",
    "                    return None\n",
    "                else:\n",
    "                    content = await page.text_content(\"body\")  # Get the text content directly from the body\n",
    "                    if content and \"Page not found\" not in content:\n",
    "                        file_path = f\"{output_dir}/{pdb_id}_consurf_summary.txt\"\n",
    "                        with open(file_path, \"w\") as file:\n",
    "                            file.write(content)\n",
    "                        print(f\"Summary for {pdb_id} chain {chain} saved successfully at {file_path}.\")\n",
    "                        return content\n",
    "                    else:\n",
    "                        print(f\"Content for {pdb_id} chain {chain} not found or invalid.\")\n",
    "                        return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdb_id} chain {chain}: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                await browser.close()\n",
    "\n",
    "    async def main():\n",
    "        tasks = []\n",
    "        for pdb_id in pdb_ids:\n",
    "            for chain in chains:\n",
    "                tasks.append(scrape_consurf(pdb_id, chain))\n",
    "        \n",
    "        # Run all tasks concurrently and gather their results\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        for pdb_id, result in zip(pdb_ids, results):\n",
    "            if isinstance(result, Exception):\n",
    "                print(f\"Error processing {pdb_id}: {result}\")\n",
    "            elif result:\n",
    "                print(f\"Summary for {pdb_id} saved successfully.\")\n",
    "            else:\n",
    "                print(f\"No content saved for {pdb_id}.\")\n",
    "\n",
    "    # Helper function to run the asyncio code in the correct context\n",
    "    def run_async_code():\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "            if loop.is_running():\n",
    "                return loop.create_task(main())\n",
    "        except RuntimeError:\n",
    "            return asyncio.run(main())\n",
    "\n",
    "    run_async_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main function'''\n",
    "\n",
    "def pdb_to_n_neighbour_vector(pdb_file_path, angstrom):\n",
    "    # convert_cif_to_pdb(cif_file_path, pdb_file_path)\n",
    "    amino_vector = []\n",
    "    title = pdb_file_path.split('/')[-1].split('.')[0][3:].upper()\n",
    "\n",
    "    atom_info_list = extract_pdb_info(pdb_file_path)\n",
    "\n",
    "    amino_acid_info_count = find_nearest_neighbors(atom_info_list, angstrom)\n",
    "\n",
    "    print(f\"Number of Amino Acids: {len(amino_acid_info_count)}\")\n",
    "    conservation_scores = extract_conservation_score(filename=f\"conservation_scores/{title}_consurf_summary.txt\")\n",
    "    print(conservation_scores)\n",
    "\n",
    "    if conservation_scores is not None:\n",
    "        merged_residues = merge_residue_data(conservation_scores, amino_acid_info_count)\n",
    "        print(merged_residues)\n",
    "        plot_and_calculate_correlation(merged_residues, title, angstrom)\n",
    "    else:\n",
    "        print(\"Conservation scores not found, skipping merge_residue_data. Going straight to plotting.\")\n",
    "        merged_residues = amino_acid_info_count\n",
    "        plot_and_calculate_correlation(merged_residues, title, angstrom)\n",
    "\n",
    "    return merged_residues\n",
    "\n",
    "def merge_to_ref_seq(ref_seq, completed):\n",
    "    # Convert ref_seq to dictionary for efficient lookup\n",
    "    ref_seq_dict = {entry['residue_id']: entry for entry in ref_seq}\n",
    "\n",
    "    for entry in completed:\n",
    "        residue_id = entry['residue_id']\n",
    "        if residue_id in ref_seq_dict:\n",
    "            ref_seq_entry = ref_seq_dict[residue_id]\n",
    "            if 'neighbour_count' in entry:\n",
    "                ref_seq_entry['neighbour_count'] = entry['neighbour_count']\n",
    "            if 'conservation_score' in entry:\n",
    "                ref_seq_entry['conservation_score'] = entry['conservation_score']\n",
    "        else:\n",
    "            ref_seq_dict[residue_id] = {\n",
    "                'residue_id': residue_id,\n",
    "                'residue_name': entry['residue_name'],\n",
    "                'neighbour_count': entry.get('neighbour_count', 0),\n",
    "                'conservation_score': entry.get('conservation_score')\n",
    "            }\n",
    "\n",
    "    # Convert back to list\n",
    "    updated_ref_seq = list(ref_seq_dict.values())\n",
    "    return updated_ref_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_file_path = ['./ASPG_pdb/1apy.pdb', './ASPG_pdb/1apz.pdb', './ASPG_pdb/1ayy.pdb', './ASPG_pdb/1p4k.pdb', './ASPG_pdb/1p4v.pdb', './ASPG_pdb/2gac.pdb', './ASPG_pdb/2gaw.pdb', './ASPG_pdb/2gl9.pdb', './ASPG_pdb/3ljq.pdb', './ASPG_pdb/4r4y.pdb', './ASPG_pdb/5v2i.pdb', './ASPG_pdb/9gaa.pdb', './ASPG_pdb/9gac.pdb', './ASPG_pdb/9gaf.pdb']\n",
    "\n",
    "download_conservation_scores(pdb_file_path, chains = ['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_seq = './ASPG_pdb/AF-Q7L266-F1-model_v4.pdb'\n",
    "ref_seq = pdb_to_n_neighbour_vector(ref_seq, angstrom = 4)\n",
    "\n",
    "for pdb_file in pdb_file_path:\n",
    "        \n",
    "        completed = pdb_to_n_neighbour_vector(pdb_file, angstrom = 4)\n",
    "        updated_ref_seq = merge_to_ref_seq(ref_seq, completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated_ref_seq)\n",
    "plot_and_calculate_correlation(updated_ref_seq, title='SIRT1-Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
