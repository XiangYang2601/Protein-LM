{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the libraries to be installed before using this code. Use `pip install` to install the following libraries: <br>\n",
    "```\n",
    "biopython\n",
    "matplotlib\n",
    "playwright\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import PDB\n",
    "import time\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function downloads the conservation score of a specific pdb file using __[Consurf Database](https://consurfdb.tau.ac.il/)__. <br>\n",
    "\n",
    "Input: `pdb_file_path`, `chain_id` (e.g. A, B, C, D) and `protein_id` (e.g. 2PE4)<br>\n",
    "Output: Folder named \"<protein_id>_conservation_scores.\" Each conservation score file is named with its respective PDB ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "def download_conservation_scores(pdb_file_paths, protein_id, molecular_name):\n",
    "    \"\"\"\n",
    "    Downloads conservation score from Consurf Database using specific PDB file\n",
    "\n",
    "    Input:\n",
    "    - pdb_file_paths (list of str): List of PDB file paths\n",
    "    - protein_id (str): Protein ID\n",
    "    - molecular_name (str): Molecular name to extract chain IDs\n",
    "\n",
    "    Output:\n",
    "    - Folder with conservation score in txt format, file named with PDB ID \n",
    "    \"\"\"\n",
    "    pattern = r'pdb(\\w+)\\.pdb'\n",
    "    pdb_ids = [match.group(1).upper() for file_path in pdb_file_paths if (match := re.search(pattern, file_path))]\n",
    "    print(pdb_ids)\n",
    "    \n",
    "    # Ensure the directory for saving files exists\n",
    "    output_dir = f\"{protein_id}_outputs/{protein_id}_conservation_scores\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    async def scrape_consurf(pdb_id, chain):\n",
    "        url = f\"https://consurfdb.tau.ac.il/DB/{pdb_id.upper()}{chain}/{pdb_id.upper()}{chain}_consurf_summary.txt\"\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch()\n",
    "            page = await browser.new_page()\n",
    "            try:\n",
    "                await page.goto(url, timeout=60000)  # Increase timeout to 60 seconds\n",
    "                content = await page.content()  # Get the content of the page\n",
    "                if \"Page not found\" in content:\n",
    "                    print(f\"Error: Page not found for {pdb_id} chain {chain}\")\n",
    "                    return None\n",
    "                else:\n",
    "                    content = await page.text_content(\"body\")  # Get the text content directly from the body\n",
    "                    if content and \"Page not found\" not in content:\n",
    "                        file_path = f\"{output_dir}/{pdb_id}{chain}_consurf_summary.txt\"\n",
    "                        with open(file_path, \"w\") as file:\n",
    "                            file.write(content)\n",
    "                        print(f\"Summary for {pdb_id} chain {chain} saved successfully at {file_path}.\")\n",
    "                        return content\n",
    "                    else:\n",
    "                        print(f\"Content for {pdb_id} chain {chain} not found or invalid.\")\n",
    "                        return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdb_id} chain {chain}: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                await browser.close()\n",
    "\n",
    "    async def main():\n",
    "        tasks = []\n",
    "        for pdb_file in pdb_file_paths:\n",
    "            pdb_id = re.search(pattern, pdb_file).group(1).upper()\n",
    "            chain_ids = extract_chain_ids(pdb_file, molecular_name)\n",
    "            print(f\"Chain IDs for {pdb_id}: {chain_ids}\")\n",
    "            for chain in chain_ids:\n",
    "                tasks.append(scrape_consurf(pdb_id, chain))\n",
    "        \n",
    "        # Run all tasks concurrently and gather their results\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        for pdb_file, result in zip(pdb_file_paths, results):\n",
    "            pdb_id = re.search(pattern, pdb_file).group(1).upper()\n",
    "            if isinstance(result, Exception):\n",
    "                print(f\"Error processing {pdb_id}: {result}\")\n",
    "            elif result:\n",
    "                print(f\"Summary for {pdb_id} saved successfully.\")\n",
    "            else:\n",
    "                print(f\"No content saved for {pdb_id}.\")\n",
    "\n",
    "    # Helper function to run the asyncio code in the correct context\n",
    "    def run_async_code():\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "            if loop.is_running():\n",
    "                return loop.create_task(main())\n",
    "        except RuntimeError:\n",
    "            return asyncio.run(main())\n",
    "\n",
    "    run_async_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function extracts the conservation scores from the downloaded conservation score files and saves the extracted scores as a list, each list item contains 3 dict objects: <i>residue_id</i>, <i>residue_name</i> and <i>conservation_score</i>. <br>\n",
    "\n",
    "Input: `filename`, `chain_id` <br>\n",
    "Output: List of dictionaries with residue information and conservation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conservation_score(filename, chain_id):\n",
    "    \"\"\"\n",
    "    Extracts conservation scores from a file, considering the specific chain ID.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): Path to the conservation score file\n",
    "    - chain_id (str): Chain ID for the PDB file\n",
    "\n",
    "    Returns:\n",
    "    - List of dictionaries with residue information and conservation scores\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                # Skip header lines and lines below confidence cut-off\n",
    "                if re.match(r'\\s*\\d+', line):\n",
    "                    columns = line.split()\n",
    "                    pos = int(columns[0])\n",
    "                    score = float(columns[3])\n",
    "\n",
    "                    # Check if there is a 3LATOM value, otherwise continue\n",
    "                    if len(columns) > 6 and ':' in columns[2]:\n",
    "                        residue_id = re.findall(r'\\d+', columns[2])[0]  # Extract numbers from the string\n",
    "                        residue_id = int(residue_id)  # Convert extracted string to integer\n",
    "                        residue_name = re.findall(r'[A-Za-z]+', columns[2])[0]  # Extract letters from the string\n",
    "                        result.append({\n",
    "                            'residue_id': residue_id,\n",
    "                            'residue_name': residue_name,\n",
    "                            'conservation_score': score,\n",
    "                            'chain_id': chain_id\n",
    "                        })\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function extracts chain ids from a PDB file that correspond to a given molecular name. <br>\n",
    "\n",
    "Input: `pdb_file` String, PDB file path, `molecular_name` String, molecular name to search for in PDB file <br>\n",
    "Output: `chain_ids_list` List of chain IDs that contain molecular name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chain_ids(pdb_file, molecular_name):\n",
    "    \"\"\"\n",
    "    Extracts chain ids from a PDB file that correspond to a given molecular name.\n",
    "    \n",
    "    Parameters:\n",
    "    - pdb_file (str): Path to the PDB file.\n",
    "    - molecular_name (str): Molecular name to search for in the PDB file.\n",
    "    \n",
    "    Returns:\n",
    "    - List of chain IDs containing the specified molecular name.\n",
    "    \"\"\"\n",
    "    chain_ids = set()  # Use a set to automatically handle duplicates\n",
    "    with open(pdb_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if molecular_name in line:\n",
    "                # The line immediately after contains chain IDs\n",
    "                if i + 1 < len(lines):\n",
    "                    chain_line = lines[i + 1]\n",
    "                    if \"CHAIN:\" in chain_line:\n",
    "                        # Strip whitespace and semicolons, then split by commas\n",
    "                        ids = chain_line.split(\"CHAIN:\")[1].strip().strip(';').split(',')\n",
    "                        chain_ids.update(id.strip(';').strip() for id in ids)\n",
    "    chain_ids_list = list(chain_ids)\n",
    "    # print(f\"Extracted chain IDs for {molecular_name} in {pdb_file}: {chain_ids_list}\")\n",
    "    return chain_ids_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads a PDB file and extracts the 3D spacial coordinates and amino acids.<br>\n",
    "\n",
    "Input: `pdb_file_path` String, PDB file path<br>\n",
    "Output: `atom_info_list` A list of dictionaries, each dictionary containing information about each atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdb_info(pdb_file_path):\n",
    "    \"\"\"\n",
    "    Reads a PDB file and extracts 3D spatial coordinates and amino acids,\n",
    "    applying residue ID corrections based on DBREF entries if available.\n",
    "\n",
    "    Input:\n",
    "    - pdb_file_path (str): Path to the PDB file\n",
    "\n",
    "    Output:\n",
    "    - atom_info_list: A list of dictionaries, each containing information about each atom\n",
    "    - chain_offsets: Dictionary with chain IDs as keys and residue ID offsets as values\n",
    "    \"\"\"\n",
    "    # Create a PDBParser object\n",
    "    parser = PDB.PDBParser(PERMISSIVE=1)\n",
    "    \n",
    "    # Parse the structure from the PDB file\n",
    "    structure = parser.get_structure('protein', pdb_file_path)\n",
    "    \n",
    "    # Dictionary to hold chain offsets from DBREF entries\n",
    "    chain_offsets = {}\n",
    "\n",
    "    # Read the PDB file line by line to find DBREF entries\n",
    "    with open(pdb_file_path, 'r') as pdb_file:\n",
    "        for line in pdb_file:\n",
    "            if line.startswith(\"DBREF\"):\n",
    "                parts = line.split()\n",
    "                chain_id = parts[2]\n",
    "                start_residue_pdb = int(parts[3])\n",
    "                start_residue_db = int(parts[8])\n",
    "                # Calculate offset\n",
    "                offset = start_residue_db - start_residue_pdb\n",
    "                chain_offsets[chain_id] = offset\n",
    "\n",
    "    # List to hold the extracted information\n",
    "    atom_info_list = []\n",
    "\n",
    "    # Extract information from the structure\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            offset = chain_offsets.get(chain.id, 0)  # Default offset is 0 if not found\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    # Apply offset to the residue ID\n",
    "                    corrected_residue_id = residue.id[1] + offset\n",
    "                    atom_info = {\n",
    "                        'model_id': model.id,\n",
    "                        'chain_id': chain.id,\n",
    "                        'residue_name': residue.resname,\n",
    "                        'residue_id': corrected_residue_id,\n",
    "                        'atom_name': atom.name,\n",
    "                        'atom_coords': atom.coord.tolist()\n",
    "                    }\n",
    "                    atom_info_list.append(atom_info)\n",
    "    # print(chain_offsets)\n",
    "\n",
    "    return atom_info_list, chain_offsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aligns sequences from the reference and PDB sequences based on residue IDs, and sorts the columns by chain ID for each PDB file. <br>\n",
    "\n",
    "Input: `ref_seq`, `pdb_sequences` <br>\n",
    "Output: Pandas DataFrame with aligned sequences, columns sorted alphabetically by chain ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sequences(ref_seq, pdb_sequences):\n",
    "    \"\"\"\n",
    "    Aligns sequences from the reference and PDB sequences based on residue IDs,\n",
    "    and sorts the columns by chain ID for each PDB file.\n",
    "\n",
    "    Parameters:\n",
    "    - ref_seq (list of dicts): Reference sequence residues.\n",
    "    - pdb_sequences (dict): Dictionary with PDB sequence titles and chain residues.\n",
    "\n",
    "    Returns:\n",
    "    - Pandas DataFrame with aligned sequences, with columns sorted alphabetically by chain ID.\n",
    "    \"\"\"\n",
    "    # Extract residue IDs from the reference sequence\n",
    "    ref_residue_ids = [residue['residue_id'] for residue in ref_seq]\n",
    "\n",
    "    # Initialize the DataFrame with the reference sequence\n",
    "    df = pd.DataFrame(ref_residue_ids, columns=['residue_id'])\n",
    "    df['ref_seq'] = [residue['residue_name'] for residue in ref_seq]\n",
    "\n",
    "    # Collect columns to be added to DataFrame\n",
    "    columns_to_add = []\n",
    "\n",
    "    # Align each PDB sequence to the reference sequence\n",
    "    for pdb_title, chain_residues in pdb_sequences.items():\n",
    "        for chain_id, residues in chain_residues.items():\n",
    "            pdb_dict = {res['residue_id']: res['residue_name'] for res in residues}\n",
    "            column_name = f\"{pdb_title}_{chain_id}\"\n",
    "            df[column_name] = df['residue_id'].apply(lambda x: pdb_dict.get(x, '-'))\n",
    "            columns_to_add.append(column_name)\n",
    "\n",
    "    # Sort columns alphabetically, keeping 'residue_id' and 'ref_seq' at the beginning\n",
    "    sorted_columns = sorted(columns_to_add)\n",
    "    sorted_columns = ['residue_id', 'ref_seq'] + sorted_columns\n",
    "    df = df[sorted_columns]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aligns and compares sequences from the reference and PDB files, without handling offsets and outputs a csv with aligned sequences.(since extract_pdb_info already handles offsets). <br>\n",
    "\n",
    "Input: `ref_seq` String, reference sequence file path, `pdb_file_paths` List of pdb file paths, `molecular_name` <br>\n",
    "Output: Pandas DataFrame with aligned sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_pdb(ref_seq, pdb_file_paths, protein_id, molecular_name):\n",
    "    \"\"\"\n",
    "    Aligns and compares sequences from the reference and PDB files, without handling offsets (since extract_pdb_info already handles offsets).\n",
    "\n",
    "    Parameters:\n",
    "    - ref_seq (str): Path to the reference PDB file.\n",
    "    - pdb_file_paths (list of str): List of PDB file paths to compare.\n",
    "    - molecular_name (str): Molecular name to search for chain IDs.\n",
    "\n",
    "    Returns:\n",
    "    - Pandas DataFrame with aligned sequences.\n",
    "    \"\"\"\n",
    "    # Extract chain IDs and residue info from the reference sequence\n",
    "    ref_atoms, ref_chain_offsets = extract_pdb_info(ref_seq)\n",
    "\n",
    "    ref_chain_ids = extract_chain_ids(ref_seq, molecular_name)\n",
    "    ref_residues = [\n",
    "        {\n",
    "            'residue_id': atom['residue_id'],\n",
    "            'residue_name': atom['residue_name']\n",
    "        }\n",
    "        for atom in ref_atoms\n",
    "        if atom['atom_name'] == 'CA'\n",
    "    ]\n",
    "    \n",
    "    if len(ref_residues) == 0:\n",
    "        raise ValueError(\"No residues extracted from reference sequence. Please check the PDB file and chain IDs.\")\n",
    "\n",
    "    # Extract residue information from each PDB file\n",
    "    pdb_sequences = {}\n",
    "    for pdb_file in pdb_file_paths:\n",
    "        pdb_title = os.path.basename(pdb_file).replace('.pdb', '')\n",
    "        chain_ids = extract_chain_ids(pdb_file, molecular_name)\n",
    "        \n",
    "        # Extract atom info from PDB\n",
    "        pdb_atoms, pdb_chain_offsets = extract_pdb_info(pdb_file)\n",
    "\n",
    "        # Do not adjust residue IDs based on offsets\n",
    "        chain_residues = {}\n",
    "        for chain_id in chain_ids:\n",
    "            residues = [\n",
    "                {\n",
    "                    'residue_id': atom['residue_id'],\n",
    "                    'residue_name': atom['residue_name']\n",
    "                }\n",
    "                for atom in pdb_atoms\n",
    "                if atom['atom_name'] == 'CA' and atom['chain_id'] == chain_id\n",
    "            ]\n",
    "            chain_residues[chain_id] = residues\n",
    "        \n",
    "        pdb_sequences[pdb_title] = chain_residues\n",
    "\n",
    "    # Align the sequences\n",
    "    df = align_sequences(ref_residues, pdb_sequences)\n",
    "    \n",
    "    # Output the DataFrame to a CSV file (optional)\n",
    "    output_file = os.path.join(f'{protein_id}_outputs', f'{protein_id}_aligned_sequences.csv')\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Aligned sequences have been written to {output_file}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds the N-neighbours closest to the Alpha Carbon atoms, using brute force with Euclidean distance. It counts the number of neighbours and also collects the `residue_id` of its neighbours<br>\n",
    "\n",
    "Input: `data` List of dictionaries, `angstrom` Float, Distance threshold for neighbour counting <br>\n",
    "Output: `data` List of dictionaries, containing only 'CA' atoms with `neighbour_count` and `neighbours` field and without `model_id`, `atom_name` and `atom_coords` keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbours(data, angstrom, chain_ids):\n",
    "    \"\"\"\n",
    "    Find the n-neighbors closest to the 'CA' atoms in the data using brute force with Euclidean distance.\n",
    "\n",
    "    Input:\n",
    "    - data: list of dictionaries, each containing atom information\n",
    "    - angstrom (float): distance threshold for neighbour counting\n",
    "    - chain_ids: list of chain IDs to include in the neighbor search\n",
    "\n",
    "    Output:\n",
    "    - ca_atoms_neighbour: list of dictionaries, containing only 'CA' atoms with updated 'neighbour_count' field,\n",
    "      and without 'model_id', 'atom_name', and 'atom_coords' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter data to only include 'CA' atoms in the specified chain_ids\n",
    "    chain_ca_atoms = {}\n",
    "    for atom in data:\n",
    "        chain_id = atom.get('chain_id')\n",
    "        if chain_id in chain_ids and atom.get('atom_name') == 'CA':\n",
    "            if chain_id not in chain_ca_atoms:\n",
    "                chain_ca_atoms[chain_id] = []\n",
    "            chain_ca_atoms[chain_id].append(atom)\n",
    "\n",
    "    all_ca_atoms_neighbour = []\n",
    "    \n",
    "    for chain_id, ca_atoms in chain_ca_atoms.items():\n",
    "        # Extract and verify coordinates\n",
    "        ca_coords = np.array([atom['atom_coords'] for atom in ca_atoms])\n",
    "        \n",
    "        # Calculate distances between 'CA' atoms\n",
    "        distances = scipy.spatial.distance.cdist(ca_coords, ca_coords, 'euclidean')\n",
    "        \n",
    "        # Use 'residue_id' for DataFrame indexing\n",
    "        residue_ids = [atom.get('residue_id', f\"residue_{i}\") for i, atom in enumerate(ca_atoms)]\n",
    "        \n",
    "        # Convert distance matrix to pandas DataFrame\n",
    "        distance_df = pd.DataFrame(distances, index=residue_ids, columns=residue_ids)\n",
    "        \n",
    "        # Format distance matrix to 3 decimal places\n",
    "        distance_df = distance_df.round(3)\n",
    "\n",
    "        # Count neighbors within a distance of 'angstrom' units for each 'CA' atom\n",
    "        for index, ca_point in enumerate(ca_coords):\n",
    "            neighbour_count = np.sum((distances[index] <= angstrom) & (distances[index] != 0))\n",
    "            ca_atoms[index]['neighbour_count'] = neighbour_count\n",
    "            \n",
    "            neighbour_ids = distance_df.columns[(distances[index] <= angstrom) & (distances[index] != 0)].tolist()\n",
    "            ca_atoms[index]['neighbours'] = neighbour_ids\n",
    "\n",
    "        \n",
    "        # Remove specified keys from the 'CA' atoms\n",
    "        keys_to_remove = ['model_id', 'atom_name', 'atom_coords']\n",
    "        for atom in ca_atoms:\n",
    "            for key in keys_to_remove:\n",
    "                atom.pop(key, None)\n",
    "        \n",
    "        # Add chain ID to each atom dictionary\n",
    "        for atom in ca_atoms:\n",
    "            atom['chain_id'] = chain_id\n",
    "        \n",
    "        all_ca_atoms_neighbour.extend(ca_atoms)\n",
    "    # print(all_ca_atoms_neighbour)\n",
    "    return all_ca_atoms_neighbour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds the hydrophobicity of the amino acid. Hydrophobicity values are taken from the __[IMGT Scale](https://www.imgt.org/IMGTeducation/Aide-memoire/_UK/aminoacids/abbreviation.html)__.<br>\n",
    "\n",
    "Input: `aa_residue` Dictionary containing `residue_name`<br>\n",
    "Output: `aa_residue` Dictionary with corresponding `hydrophobicity` information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hydrophobicity(aa_residue):\n",
    "    \"\"\"\n",
    "    Assign hydrophobicity values to an amino acid, using IMGT Scale\n",
    "\n",
    "    Input:\n",
    "    - aa_residue: Dict object with 'residue_name'\n",
    "\n",
    "    Ouput:\n",
    "    - aa_residue: Dict object with 'hydrophobicity' key\n",
    "    \"\"\"\n",
    "    hydrophobicity_scale = {\"ALA\": 1.8, \"ARG\": -4.5, \"ASN\": -3.5, \"ASP\": -3.5, \"CYS\": 2.5, \"GLN\": -3.5, \"GLU\": -3.5,\n",
    "            \"GLY\": -0.4, \"HIS\": -3.2, \"ILE\": 4.5, \"LEU\": 3.8, \"LYS\": -3.9, \"MET\": 1.9, \"PHE\": 2.8,\n",
    "            \"PRO\": -1.6, \"SER\": -0.8, \"THR\": -0.7, \"TRP\": -0.9, \"TYR\": -1.3, \"VAL\": 4.2}\n",
    "        \n",
    "    if aa_residue[\"residue_name\"] in hydrophobicity_scale:\n",
    "        aa_residue['hydrophobicity'] = hydrophobicity_scale[aa_residue[\"residue_name\"]]\n",
    "    else:\n",
    "        aa_residue['hydrophobicity'] = None\n",
    "    \n",
    "    return aa_residue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds the volume of the amino acid. Volume information is taken from the __[IMGT Scale](https://www.imgt.org/IMGTeducation/Aide-memoire/_UK/aminoacids/abbreviation.html)__.<br>\n",
    "\n",
    "Input: `aa_residue` Dictionary containing `residue_name`<br>\n",
    "Output: `aa_residue` Dictionary with corresponding `volume` information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_volume(aa_residue):\n",
    "    \"\"\"\n",
    "    Assign volume to an amino acid, using IMGT Scale\n",
    "\n",
    "    Input:\n",
    "    - aa_residue: Dict object with 'residue_name'\n",
    "\n",
    "    Ouput:\n",
    "    - aa_residue: Dict object with 'volume' key\n",
    "    \"\"\"\n",
    "    volume_scale = {\"ALA\": 88.6, \"ARG\": 173.4, \"ASN\": 114.1, \"ASP\": 111.1, \"CYS\": 108.5, \"GLN\": 143.8, \"GLU\": 138.4,\n",
    "            \"GLY\": 60.4, \"HIS\": 153.2, \"ILE\": 166.7, \"LEU\": 166.7, \"LYS\": 168.6, \"MET\": 162.9, \"PHE\": 189.9,\n",
    "            \"PRO\": 112.7, \"SER\": 89.0, \"THR\": 116.1, \"TRP\": 227.8, \"TYR\": 193.6, \"VAL\": 140.0}\n",
    "        \n",
    "    if aa_residue['residue_name'] in volume_scale:\n",
    "        aa_residue['volume'] = volume_scale[aa_residue['residue_name']]\n",
    "    else:\n",
    "        aa_residue['volume'] = None\n",
    "    \n",
    "    return aa_residue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function classifies the proton accepting & donating ability of the amino acids. Amino acids that are non-donors and non-acceptors are given a value of 0, Either Donor or Acceptor is given a value of 1 and Both Donor and Acceptor are given a value of 2.5. Ref from: __[IMGT](https://www.imgt.org/IMGTeducation/Aide-memoire/_UK/aminoacids/charge/index.html#hydrogen)__ <br>\n",
    "\n",
    "Input: `aa_residue` Dictionary containing `residue_name`<br>\n",
    "Output: `aa_residue` Dictionary with corresponding `proton_donor` information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proton_donor(aa_residue):\n",
    "    \"\"\"\n",
    "    Classifies amino acid based on proton accepting/donating ability\n",
    "\n",
    "    Input:\n",
    "    - aa_residue: Dict object with 'residue_name'\n",
    "\n",
    "    Ouput:\n",
    "    - aa_residue: Dict object with 'proton_donor' key\n",
    "    \"\"\"\n",
    "    # Protons donor given 1, Protons acceptors given 1, Both donor and acceptor given 2.5, Neither donor nor receiver given 0\n",
    "    proton_scale = {\"ALA\": 0, \"ARG\": 1, \"ASN\": 2.5, \"ASP\": 1, \"CYS\": 0, \"GLN\": 2.5, \"GLU\": 1,\n",
    "            \"GLY\": 0, \"HIS\": 2.5, \"ILE\": 0, \"LEU\": 0, \"LYS\": 1, \"MET\": 0, \"PHE\": 0,\n",
    "            \"PRO\": 0, \"SER\": 2.5, \"THR\": 2.5, \"TRP\": 1, \"TYR\": 2.5, \"VAL\": 0}\n",
    "        \n",
    "    if aa_residue['residue_name'] in proton_scale:\n",
    "        aa_residue['proton_donor'] = proton_scale[aa_residue['residue_name']]\n",
    "    else:\n",
    "        aa_residue['proton_donor'] = None\n",
    "    \n",
    "    return aa_residue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function merges the `neighbour_count` and `conservation_score` information. It then calls the `find_hydrophobicity`, `find_volume` and `find_proton_donor` functions to assign the respective information to each residue. <br>\n",
    "\n",
    "Input: `conservation_scores` List of Dictionaries with Conservation Score, `neighbour_counts` List of Dictionaries with Neighbour Count<br>\n",
    "Output: `merged_data` List of Dictionaries with `residue_id`, `residue_name`, `conservation_score`, `neighbour_count`, `neighbours`, `hydrophobicity`, `volume` and `proton_donor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_residue_data(conservation_scores, neighbour_counts, chain_id, chain_offsets):\n",
    "    \"\"\"\n",
    "    Merges conservation score and neighbour count, then assigns hydrophobicity, volume and proton donor/acceptor\n",
    "    considering the DBREF displacement for conservation scores only.\n",
    "\n",
    "    Input:\n",
    "    - conservation_scores: List of dictionaries with conservation score\n",
    "    - neighbour_counts: List of dictionaries with neighbour count\n",
    "    - chain_id: The chain ID to differentiate between chains\n",
    "    - chain_offsets: Dictionary with chain IDs as keys and residue ID offsets as values\n",
    "\n",
    "    Output:\n",
    "    - merged_data: List of dictionaries with residue id, residue name, conservation score, neighbour count, neighbours, hydrophobicity, volume and proton donor/acceptor\n",
    "    \"\"\"\n",
    "    # Create dictionaries to index the data by residue_id, residue_name, and chain_id\n",
    "    neighbour_dict = {\n",
    "        (item['residue_id'], item['residue_name'], item['chain_id']): item\n",
    "        for item in neighbour_counts\n",
    "    }\n",
    "    conservation_dict = {\n",
    "        (item['residue_id'], item['residue_name'], item['chain_id']): item\n",
    "        for item in conservation_scores\n",
    "    }\n",
    "\n",
    "    # Determine all unique residue identifiers, including chain_id\n",
    "    all_residues = set(neighbour_dict.keys()).union(conservation_dict.keys())\n",
    "\n",
    "    # List to hold merged data\n",
    "    merged_data = []\n",
    "\n",
    "    # Get the displacement (offset) for the current chain_id\n",
    "    displacement = chain_offsets.get(chain_id, 0)\n",
    "\n",
    "    # Merge data from conservation_scores with neighbour_counts\n",
    "    for residue_id, residue_name, chain in all_residues:\n",
    "        # Adjust residue ID for conservation scores based on displacement\n",
    "        adjusted_residue_id = residue_id + displacement if (residue_id, residue_name, chain) in conservation_dict else residue_id\n",
    "\n",
    "        # Create a dictionary entry for the residue\n",
    "        merged_entry = {\n",
    "            'residue_id': adjusted_residue_id,\n",
    "            'residue_name': residue_name,\n",
    "            'chain_id': chain,\n",
    "            'conservation_score': conservation_dict.get((residue_id, residue_name, chain), {}).get('conservation_score', None),\n",
    "            'neighbour_count': neighbour_dict.get((residue_id, residue_name, chain), {}).get('neighbour_count', None),\n",
    "            'neighbours': neighbour_dict.get((residue_id, residue_name, chain), {}).get('neighbours')\n",
    "        }\n",
    "        \n",
    "        # Add additional attributes\n",
    "        find_hydrophobicity(merged_entry)\n",
    "        find_volume(merged_entry)\n",
    "        find_proton_donor(merged_entry)\n",
    "\n",
    "        # Append to the merged data list\n",
    "        merged_data.append(merged_entry)\n",
    "    \n",
    "    # Sort the merged data by chain_id and residue_id\n",
    "    merged_data.sort(key=lambda x: (x['chain_id'], x['residue_id']))\n",
    "\n",
    "    return merged_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calls `extract_pdb_info`, `find_nearest_neighbors`. `extract_conservation_score` and `merge_residue_data`<br>\n",
    "\n",
    "Input: `pdb_file_path`, `angstrom`, `protein_id`<br>\n",
    "Ouput: `merged_data` with all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb_to_compiled_vector(pdb_file_path, angstrom, protein_id, molecular_name):\n",
    "    \"\"\"\n",
    "    For each pdb sequence, outputs the combined data with all 5 parameters\n",
    "\n",
    "    Input:\n",
    "    - pdb_file_path (str): Path to the PDB file\n",
    "    - angstrom: float, distance threshold for neighbour counting\n",
    "    - protein_id (str): Protein ID\n",
    "    - molecular_name (str): Protein name\n",
    "\n",
    "    Output:\n",
    "    - merged_residues: List of dictionaries with residue id, residue name, conservation score, neighbour count, neighbours, hydrophobicity, volume, and proton donor/acceptor\n",
    "    \"\"\"\n",
    "    title = pdb_file_path.split('/')[-1].split('.')[0][3:].upper()\n",
    "\n",
    "    # Extract atom information and chain offsets\n",
    "    atom_info_list, chain_offsets = extract_pdb_info(pdb_file_path)\n",
    "\n",
    "    # Extract chain IDs from the PDB file\n",
    "    chain_ids = extract_chain_ids(pdb_file_path, molecular_name)\n",
    "\n",
    "    # Find the nearest neighbours\n",
    "    neighbour_count = find_nearest_neighbours(atom_info_list, angstrom, chain_ids)\n",
    "    \n",
    "    print(f\"Number of Amino Acids ({title}): {len(neighbour_count)}\")\n",
    "    \n",
    "    merged_residues = []\n",
    "    for chain_id in chain_ids:\n",
    "        # Extract conservation scores for the current chain\n",
    "        conservation_scores = extract_conservation_score(\n",
    "            filename=f\"{protein_id}_outputs/{protein_id}_conservation_scores/{title}{chain_id}_consurf_summary.txt\",\n",
    "            chain_id=chain_id\n",
    "        )\n",
    "\n",
    "        # Merge data for the current chain using chain_offsets\n",
    "        if conservation_scores is not None:\n",
    "            merged_residues = merge_residue_data(conservation_scores, neighbour_count, chain_id, chain_offsets)\n",
    "        else:\n",
    "            print(f\"Conservation scores not found for {title} chain {chain_id}\")\n",
    "            merged_residues = merge_residue_data([], neighbour_count, chain_id, chain_offsets)\n",
    "\n",
    "    return merged_residues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function updates the consensus sequence with the information extracted from the PDB file. `neighbour_count`, `neighbours` and `conservation_score` fields are updated. <br>\n",
    "\n",
    "Input: `ref_seq` Consensus Sequence from __(https://alphafold.ebi.ac.uk/)__ with complete sequence, `Completed` Processed pdb file to be updated to `ref_seq` <br>\n",
    "Output: `updated_ref_seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_to_ref_seq(ref_seq, completed, angstrom):\n",
    "    \"\"\"\n",
    "    Updates consensus sequence with information extracted from the PDB file.\n",
    "\n",
    "    Input:\n",
    "    - ref_seq (list): List of dictionaries from the reference PDB sequence\n",
    "    - completed (list): Processed PDB file data to be updated in ref_seq\n",
    "    - angstrom (float): Distance threshold for neighbor counting\n",
    "\n",
    "    Output:\n",
    "    - updated_ref_seq: List of dictionaries\n",
    "    \"\"\"\n",
    "    # Convert ref_seq to a dictionary for efficient lookup\n",
    "    ref_seq_dict = {entry['residue_id']: entry for entry in ref_seq}\n",
    "\n",
    "    for entry in completed:\n",
    "        residue_id = entry['residue_id']\n",
    "        if residue_id in ref_seq_dict:\n",
    "            ref_seq_entry = ref_seq_dict[residue_id]\n",
    "            if 'neighbour_count' in entry and entry['neighbour_count'] is not None:\n",
    "                ref_seq_entry['neighbour_count'] = entry['neighbour_count']\n",
    "                ref_seq_entry['neighbours'] = entry['neighbours']\n",
    "            if 'conservation_score' in entry and entry['conservation_score'] is not None:\n",
    "                ref_seq_entry['conservation_score'] = entry['conservation_score']\n",
    "        else:\n",
    "            neighbour_count = entry.get('neighbour_count')\n",
    "\n",
    "            ref_seq_dict[residue_id] = {\n",
    "                'residue_id': residue_id,\n",
    "                'residue_name': entry['residue_name'],\n",
    "                'neighbour_count': neighbour_count,\n",
    "                'conservation_score': entry.get('conservation_score'),\n",
    "                'hydrophobicity': entry.get('hydrophobicity'),\n",
    "                'volume': entry.get('volume'),\n",
    "                'proton_donor': entry.get('proton_donor'),\n",
    "                'neighbours': entry.get('neighbours')\n",
    "            }\n",
    "\n",
    "    # Convert back to list\n",
    "    updated_ref_seq = list(ref_seq_dict.values())\n",
    "\n",
    "    return updated_ref_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots `neighbour_counts` and `conservation_scores` on the same plot, with `residue_id` as the x-axis. It also calculates the correlation between `neighbour_counts` and `conservation_scores`.\n",
    "\n",
    "Input: `data` List of dictionaries of residues, `angstrom`, `protein_id`, `num_residues`<br>\n",
    "Ouput: Scatter Plot containing `neighbour_counts` and `conservation_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_calculate_correlation(data, angstrom, protein_id, num_residues):\n",
    "    \"\"\"\n",
    "    Plots neighbout count and conservation scores for all residues\n",
    "\n",
    "    Input: \n",
    "    - data: List of dictionaries\n",
    "    - angstrom\n",
    "    - protein_id\n",
    "    - num_residues\n",
    "\n",
    "    Output:\n",
    "    - scatter plot: y axes are neighbour count and conservation score, x axis is residue id\n",
    "    - caculated correlation between neighbour counts and conservations scores\n",
    "    \"\"\"\n",
    "    # Extract data for plotting\n",
    "    residue_ids = [entry['residue_id'] for entry in data]\n",
    "    neighbour_counts = [entry['neighbour_count'] for entry in data]\n",
    "    \n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot neighbour counts\n",
    "    plt.scatter(residue_ids, neighbour_counts, color='green', label='Neighbour Count')\n",
    "    \n",
    "    # Check and plot conservation scores if available\n",
    "    conservation_scores = [entry['conservation_score'] for entry in data if 'conservation_score' in entry and entry['conservation_score'] is not None]\n",
    "    if conservation_scores:\n",
    "        valid_entries = [entry for entry in data if 'conservation_score' in entry and entry['conservation_score'] is not None]\n",
    "        valid_residue_ids = [entry['residue_id'] for entry in valid_entries]\n",
    "        valid_neighbour_counts = [entry['neighbour_count'] for entry in valid_entries]\n",
    "        \n",
    "        plt.scatter(valid_residue_ids, conservation_scores, color='blue', label='Conservation Score')\n",
    "        \n",
    "        # Calculate and print the correlation\n",
    "        if len(valid_residue_ids) == len(conservation_scores):\n",
    "            correlation = np.corrcoef(conservation_scores, valid_neighbour_counts)[0, 1]\n",
    "            print(f\"Correlation between Conservation Score and Neighbour Count: {correlation}\")\n",
    "        else:\n",
    "            print(\"Mismatch in lengths of valid_residue_ids and conservation_scores\")\n",
    "    else:\n",
    "        print(\"Conservation scores not found. Plotting only Neighbour Count.\")\n",
    "\n",
    "    plt.xlabel('Residue ID')\n",
    "    plt.ylabel('Score / Count')\n",
    "    plt.title(f'{protein_id} Complete: Residue ID vs Conservation Score and Neighbour Count ({angstrom}A)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set plot limits for residue IDs\n",
    "    plt.xlim(0, num_residues)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the different parameters of individual residues as a horizontal bar chart. The x-axis is the `residue_id` and y-axis plots `hydrophobicity`, `neighbour_count`, `conservation_score`, `volume` and `proton_donor` values. A legend is provided below for identification. <br>\n",
    "\n",
    "Input: `data` List of dictionaries containing residue properties, `protein_id`, `num_residues` <br>\n",
    "Output: Summary Chart with the 5 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residue_properties(data, protein_id, num_residues=None):\n",
    "    # Limit the data to the specified number of residues\n",
    "    if num_residues is not None:\n",
    "        data = data[:num_residues]\n",
    "\n",
    "    # Extract data for plotting and replace None with 0\n",
    "    residue_ids = [entry['residue_id'] for entry in data]\n",
    "    hydrophobicities = [entry.get('hydrophobicity', 0) or 0 for entry in data]\n",
    "    conservation_scores = [entry.get('conservation_score', 0) or 0 for entry in data]\n",
    "    neighbour_counts = [entry.get('neighbour_count', 0) or 0 for entry in data]\n",
    "    volumes = [entry.get('volume', 0) or 0 for entry in data]\n",
    "    proton_donors = [entry.get('proton_donor', 0) or 0 for entry in data]\n",
    "\n",
    "    # Handle cases where lists might be empty\n",
    "    min_hydrophobicity = min(hydrophobicities)\n",
    "    max_hydrophobicity = max(hydrophobicities)\n",
    "    min_conservation = min(conservation_scores)\n",
    "    max_conservation = max(conservation_scores)\n",
    "    min_neighbour = min(neighbour_counts)\n",
    "    max_neighbour = max(neighbour_counts)\n",
    "    min_volume = min(volumes)\n",
    "    max_volume = max(volumes)\n",
    "    min_proton = min(proton_donors)\n",
    "    max_proton = max(proton_donors)\n",
    "\n",
    "    norm_hydrophobicity = mcolors.Normalize(vmin=min_hydrophobicity, vmax=max_hydrophobicity)\n",
    "    norm_conservation = mcolors.Normalize(vmin=min_conservation, vmax=max_conservation)\n",
    "    norm_neighbour = mcolors.Normalize(vmin=min_neighbour, vmax=max_neighbour)\n",
    "    norm_volume = mcolors.Normalize(vmin=min_volume, vmax=max_volume)\n",
    "    norm_proton = mcolors.Normalize(vmin=min_proton, vmax=max_proton)\n",
    "\n",
    "\n",
    "    cmap = plt.cm.coolwarm  # You can choose any colormap you prefer\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "    # Set the title of the plot\n",
    "    plt.title(f'Summary of {protein_id} Properties')\n",
    "\n",
    "    # Plot proton donors/ acceptors \n",
    "    for i, proton_donor in enumerate(proton_donors):\n",
    "        ax.barh(4.5, 1, left=i, color=cmap(norm_proton(proton_donor)), edgecolor='none', height=1)\n",
    "\n",
    "    # Plot volume    \n",
    "    for i, volume in enumerate(volumes):\n",
    "        ax.barh(3.5, 1, left=i, color=cmap(norm_volume(volume)), edgecolor='none', height=1)\n",
    "\n",
    "    # Plot hydrophobicity    \n",
    "    for i, hydrophobicity in enumerate(hydrophobicities):\n",
    "        ax.barh(2.5, 1, left=i, color=cmap(norm_hydrophobicity(hydrophobicity)), edgecolor='none', height=1)\n",
    "\n",
    "    # Plot conservation scores\n",
    "    for i, score in enumerate(conservation_scores):\n",
    "        ax.barh(1.5, 1, left=i, color=cmap(norm_conservation(score)), edgecolor='none', height=1)\n",
    "\n",
    "    # Plot neighbour counts\n",
    "    for i, count in enumerate(neighbour_counts):\n",
    "        ax.barh(0.5, 1, left=i, color=cmap(norm_neighbour(count)), edgecolor='none', height=1)\n",
    "\n",
    "    # Add colorbars to reflect each property\n",
    "    sm_proton = plt.cm.ScalarMappable(cmap=cmap, norm=norm_proton)\n",
    "    sm_volume = plt.cm.ScalarMappable(cmap=cmap, norm=norm_volume)\n",
    "    sm_hydro = plt.cm.ScalarMappable(cmap=cmap, norm=norm_hydrophobicity)\n",
    "    sm_conserv = plt.cm.ScalarMappable(cmap=cmap, norm=norm_conservation)\n",
    "    sm_neigh = plt.cm.ScalarMappable(cmap=cmap, norm=norm_neighbour)\n",
    "\n",
    "    # Create colorbar for each property\n",
    "    cbar_ax_proton = fig.add_axes([0.1, -0.1, 0.15, 0.03])\n",
    "    cbar_ax_volume = fig.add_axes([0.3, -0.1, 0.15, 0.03])\n",
    "    cbar_ax_hydro = fig.add_axes([0.5, -0.1, 0.15, 0.03])\n",
    "    cbar_ax_conserv = fig.add_axes([0.7, -0.1, 0.15, 0.03])\n",
    "    cbar_ax_neigh = fig.add_axes([0.9, -0.1, 0.15, 0.03])\n",
    "\n",
    "    fig.colorbar(sm_proton, cax=cbar_ax_proton, orientation='horizontal', fraction=0.02, pad=0.1, label='Proton Donor/Acceptor')\n",
    "    fig.colorbar(sm_volume, cax=cbar_ax_volume, orientation='horizontal', fraction=0.02, pad=0.1, label='Volume')\n",
    "    fig.colorbar(sm_hydro, cax=cbar_ax_hydro, orientation='horizontal', fraction=0.02, pad=0.1, label='Hydrophobicity')\n",
    "    fig.colorbar(sm_conserv, cax=cbar_ax_conserv, orientation='horizontal', fraction=0.02, pad=0.1, label='Conservation Score')\n",
    "    fig.colorbar(sm_neigh, cax=cbar_ax_neigh, orientation='horizontal', fraction=0.02, pad=0.1, label='Neighbour Count')\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_yticks([0.5, 1.5, 2.5, 3.5, 4.5])\n",
    "    ax.set_yticklabels(['Neighbour Count', 'Conservation Score', 'Hydrophobicity', 'Volume', 'Proton Donor/Acceptor'])\n",
    "\n",
    "    # Add x-axis labels for every 100 residues\n",
    "    x_ticks = range(0, len(residue_ids), 100)\n",
    "    x_tick_labels = [str(i) for i in x_ticks]\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function outputs the `merged_data` into a csv file.<br>\n",
    "\n",
    "Input: `data` List of Dictionaries, `protein_id`<br>\n",
    "Output: Summarised csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_csv(data, protein_id):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(f'{protein_id}_outputs', exist_ok=True)\n",
    "\n",
    "    # Create a DataFrame from the data for CSV output\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Drop the 'chain_id' column if it exists\n",
    "    if 'chain_id' in df.columns:\n",
    "        df = df.drop(columns=['chain_id'])\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_file = os.path.join(f'{protein_id}_outputs', f'{protein_id}_summary.csv')\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print confirmation\n",
    "    print(f\"Data saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following 2 functions analyse the different parameters of the protein sequence and returns residues that meet the criteria. <br>\n",
    "\n",
    "The first function sets a requirement for the 5 parameters and returns a list of amino acids that fulfil X number of criteria out of the five. <br>\n",
    "\n",
    "The second function takes a weight input for each of the 5 parameters, normalises the data to a range between 0 to 1 and returns the weighted sum from the five parameters <br>\n",
    "\n",
    "Input: `residues` List of dictionaries, `weights` (for second function), `protein_id`<br>\n",
    "Output: Analysed residues csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_protein(residues):\n",
    "    # Function to check criteria\n",
    "    def meets_criteria(residue):\n",
    "        criteria_count = 0\n",
    "\n",
    "        if residue.get('neighbour_count', 0) >= 4:\n",
    "            criteria_count += 1\n",
    "        # Skip the conservation_score criterion if it is None\n",
    "        if residue.get('conservation_score') is not None and residue.get('conservation_score') <= -0.5:\n",
    "            criteria_count += 1\n",
    "        if residue.get('hydrophobicity') is not None and (residue.get('hydrophobicity', 0) > 3 or residue.get('hydrophobicity', 0) < -3):\n",
    "            criteria_count += 1\n",
    "        if residue.get('proton_donor') is not None and residue.get('proton_donor', 0) > 2:\n",
    "            criteria_count += 1\n",
    "        if residue.get('volume') is not None and residue.get('volume', float('inf')) < 120.0:\n",
    "            criteria_count += 1\n",
    "\n",
    "        return criteria_count >= 3\n",
    "\n",
    "    # Find residues that meet the criteria\n",
    "    qualifying_residues = [residue for residue in residues if meets_criteria(residue)]\n",
    "\n",
    "    # Sort residues by residue_id\n",
    "    qualifying_residues.sort(key=lambda x: x['residue_id'])\n",
    "\n",
    "    # Extract and return the list of amino acids\n",
    "    amino_acids = [residue['residue_id'] for residue in qualifying_residues]\n",
    "\n",
    "    # Output the list\n",
    "    print(f\"Number of Amino acids that meet the criteria: {len(amino_acids)}\")\n",
    "    print(amino_acids)\n",
    "\n",
    "    return amino_acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_protein_2(residues, weights, protein_id):\n",
    "    # Ensure weights sum to 1\n",
    "    if not np.isclose(sum(weights), 1):\n",
    "        raise ValueError(\"Weights must sum to 1.\")\n",
    "\n",
    "    # Extract all parameter values\n",
    "    neighbour_counts = [residue['neighbour_count'] for residue in residues]\n",
    "    conservation_scores = [residue['conservation_score'] if residue['conservation_score'] is not None else 0 for residue in residues]\n",
    "    hydrophobicities = [residue['hydrophobicity']  if residue['hydrophobicity']is not None else 0 for residue in residues]\n",
    "    h_donors = [residue['proton_donor'] if residue['proton_donor']is not None else 0 for residue in residues]\n",
    "    volumes = [residue['volume'] if residue['volume']is not None else 0 for residue in residues]\n",
    "\n",
    "    # Normalization basis\n",
    "    max_neighbour_count = max(neighbour_counts)\n",
    "    min_neighbour_count = min(neighbour_counts)\n",
    "\n",
    "    max_conservation = max(conservation_scores)\n",
    "    min_conservation = min(conservation_scores)\n",
    "\n",
    "    max_hydrophobicity = max(abs(h) if not None else 0 for h in hydrophobicities )\n",
    "    min_hydrophobicity = -max_hydrophobicity\n",
    "\n",
    "    max_h_donor = max(h_donors)\n",
    "    min_h_donor = min(h_donors)\n",
    "\n",
    "    max_volume = max(volumes)\n",
    "    min_volume = min(volumes)\n",
    "\n",
    "    # Normalize values\n",
    "    for residue in residues:\n",
    "        # Normalized values\n",
    "        residue['norm_neighbour_count'] = (residue['neighbour_count'] - min_neighbour_count) / (max_neighbour_count - min_neighbour_count) if max_neighbour_count > min_neighbour_count else 0\n",
    "\n",
    "        residue['norm_conservation_score'] = (\n",
    "            (max_conservation - residue['conservation_score']) / (max_conservation - min_conservation)\n",
    "            if residue['conservation_score'] is not None and max_conservation > min_conservation else 0\n",
    "        )\n",
    "        # Handle None value for hydrophobicity\n",
    "        if residue['hydrophobicity'] is not None:\n",
    "            abs_hydrophobicity = abs(residue['hydrophobicity'])\n",
    "            residue['norm_hydrophobicity'] = (\n",
    "                (abs_hydrophobicity - min_hydrophobicity) / (max_hydrophobicity - min_hydrophobicity)\n",
    "                if max_hydrophobicity > min_hydrophobicity else 0\n",
    "            )\n",
    "            residue['norm_hydrophobicity'] = min(residue['norm_hydrophobicity'], 1)  # Ensure the value is not greater than 1\n",
    "        else:\n",
    "            residue['norm_hydrophobicity'] = 0  # Default for None values\n",
    "        \n",
    "        # Handle None value for proton_donor\n",
    "        if residue['proton_donor'] is not None:\n",
    "            residue['norm_proton_donor'] = (\n",
    "                (residue['proton_donor'] - min_h_donor) / (max_h_donor - min_h_donor)\n",
    "                if max_h_donor > min_h_donor else 0\n",
    "            )\n",
    "        else:\n",
    "            residue['norm_proton_donor'] = 0  # Default for None values\n",
    "        \n",
    "        if residue['volume'] is not None:\n",
    "            residue['norm_volume'] = (\n",
    "                (max_volume - residue['volume']) / (max_volume - min_volume) \n",
    "                if max_volume > min_volume else 0\n",
    "            )\n",
    "        else: residue['norm_volume'] = 0\n",
    "\n",
    "        # Weighted sum calculation\n",
    "        residue['weighted_sum'] = (\n",
    "            residue['norm_neighbour_count'] * weights[0] +\n",
    "            residue['norm_conservation_score'] * weights[1] +\n",
    "            residue['norm_hydrophobicity'] * weights[2] +\n",
    "            residue['norm_proton_donor'] * weights[3] +\n",
    "            residue['norm_volume'] * weights[4]\n",
    "        )\n",
    "\n",
    "    # Sort residues by decreasing weighted sum\n",
    "    residues.sort(key=lambda x: x['weighted_sum'], reverse=True)\n",
    "\n",
    "    # Output to CSV\n",
    "    output_path = f'{protein_id}_outputs/{protein_id}_residue_analysis.csv'\n",
    "    with open(output_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header\n",
    "        writer.writerow([\n",
    "            'Residue ID', 'Residue Name',\n",
    "            'Norm Neighbour Count', 'Norm Conservation Score', 'Norm Hydrophobicity', 'Norm Proton Donor', 'Norm Volume',\n",
    "            'Weighted Sum'\n",
    "        ])\n",
    "        # Write data\n",
    "        for residue in residues:\n",
    "            writer.writerow([\n",
    "                residue['residue_id'], residue['residue_name'],\n",
    "                residue['norm_neighbour_count'], residue['norm_conservation_score'], residue['norm_hydrophobicity'],\n",
    "                residue['norm_proton_donor'], residue['norm_volume'],\n",
    "                residue['weighted_sum']\n",
    "            ])\n",
    "\n",
    "    print(\"Residue analysis has been written to residue_analysis.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function to update for any new proteins to be analysed: <br>\n",
    "\n",
    "`pdb_file_paths`: A list of pdb file paths, pdb files downloaded from Uniprot and uploaded into \"<protein>_pdb\" folder <br>\n",
    "`ref_seq`: File path for consensus sequence, pdb file downloaded from AlphaFold DB <br>\n",
    "`protein_id`: Gene id will suffice <br>\n",
    "`num_residues`: Total number of residues, taken from AlphaFold DB where `ref_seq` is downloaded from <br>\n",
    "`weights`: Weights of the 5 parameters, need to add up to 1 <br>\n",
    "\n",
    "Outputs: <br>\n",
    "\"<protein_id>_conservation_scores\" folder with downloaded conservation scores<br>\n",
    "\"<protein_id>_aligned_sequence\" csv file<br>\n",
    "\"<protein_id>_summary\" csv file<br>\n",
    "`neighbour_count` and `conservation_score` graph<br>\n",
    "Summary chart for protein, showing 5 parameters<br>\n",
    "\"<protein_id>_residue_analysis\" csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'align_pdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.2\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#download_conservation_scores(pdb_file_paths, protein_id, molecular_name)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43malign_pdb\u001b[49m(ref_seq, pdb_file_paths, protein_id, molecular_name)\n\u001b[1;32m     24\u001b[0m ref_seq \u001b[38;5;241m=\u001b[39m pdb_to_compiled_vector(ref_seq, angstrom, protein_id, molecular_name)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRef Seq:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_seq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'align_pdb' is not defined"
     ]
    }
   ],
   "source": [
    "# Edit this\n",
    "\"\"\"\n",
    "Main Function\n",
    "\n",
    "Vairables to Update:\n",
    "- pdb_file_paths\n",
    "- ref_seq\n",
    "- protein_id\n",
    "- num_residues\n",
    "- weights\n",
    "\"\"\"\n",
    "pdb_file_paths = ['./LOX_pdb/pdb2du2.pdb', './LOX_pdb/pdb2e77.pdb', './LOX_pdb/pdb2j6x.pdb', './LOX_pdb/pdb2nli.pdb', \n",
    "                  './LOX_pdb/pdb2zfa.pdb', './LOX_pdb/pdb4rje.pdb', './LOX_pdb/pdb4yl2.pdb', './LOX_pdb/pdb5ebu.pdb', \n",
    "                  './LOX_pdb/pdb7f1y.pdb', './LOX_pdb/pdb7f20.pdb', './LOX_pdb/pdb7f21.pdb', './lOX_pdb/pdb7f22.pdb']\n",
    "ref_seq = './LOX_pdb/AF-Q44467-F1-model_v4.pdb'\n",
    "protein_id = 'LOX'\n",
    "molecular_name = 'LACTATE OXIDASE'\n",
    "num_residues = 374\n",
    "\n",
    "angstrom = 5\n",
    "weights = [0.2,0.2,0.2,0.2,0.2]\n",
    "\n",
    "#download_conservation_scores(pdb_file_paths, protein_id, molecular_name)\n",
    "align_pdb(ref_seq, pdb_file_paths, protein_id, molecular_name)\n",
    "\n",
    "ref_seq = pdb_to_compiled_vector(ref_seq, angstrom, protein_id, molecular_name)\n",
    "print(f\"Ref Seq:{ref_seq}\")\n",
    "\n",
    "for pdb_file in pdb_file_paths:\n",
    "        completed = pdb_to_compiled_vector(pdb_file, angstrom, protein_id, molecular_name)\n",
    "        updated_ref_seq = merge_to_ref_seq(ref_seq, completed, angstrom)\n",
    "\n",
    "print(f\"Updated Seq: {updated_ref_seq}\")\n",
    "\n",
    "plot_and_calculate_correlation(updated_ref_seq, angstrom, protein_id, num_residues)\n",
    "plot_residue_properties(updated_ref_seq, protein_id, num_residues)\n",
    "save_to_csv(updated_ref_seq, protein_id)\n",
    "#analyse_protein(updated_ref_seq)\n",
    "#analyse_protein_2(updated_ref_seq, weights, protein_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
